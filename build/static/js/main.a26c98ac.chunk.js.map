{"version":3,"sources":["logo.svg","DefaultImages.js","VisualAI.js","SpeechAI.js","App.js","reportWebVitals.js","index.js"],"names":["DefaultImages","RandomImageUrl","Math","floor","random","length","key","process","endpoint","console","log","visualFeatures","includesText","tags","a","filter","el","name","toLowerCase","includesHandwriting","wait","timeout","Promise","resolve","setTimeout","computerVision","url","computerVisionClient","ComputerVisionClient","ApiKeyCredentials","inHeader","urlToAnalyze","analyzeImage","analysis","readTextFromURL","text","client","STATUS_SUCCEEDED","read","result","operationID","operationLocation","split","slice","start","Date","now","status","getReadResult","analyzeResult","REACT_APP_SpeechKey","REACT_APP_SpeechEndPoint","App","useState","fileSelected","setFileSelected","setAnalysis","processing","setProcessing","handleChange","e","target","value","onFileUrlEntered","then","items","DisplayResults","src","URL","height","border","data","JSON","stringify","Analyze","type","placeholder","size","onChange","onClick","CantAnalyze","ComputerVisionIsConfigured","reportWebVitals","onPerfEntry","Function","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"8OAAe,I,uDCgBTA,EAAgB,CAhBF,oIACK,2EACT,2EACE,4GACI,sGACA,8HAGA,oIACA,oIAIC,iIACF,4IARQ,0EACD,sIA6BbC,EAJQ,WACnB,OAAOD,EAAcE,KAAKC,MAAMD,KAAKE,SAAWF,KAAKC,MAAMH,EAAcK,WCzBvEC,EAAMC,mCACNC,EAAWD,8CAEjBE,QAAQC,IAAR,gBAAqBJ,IACrBG,QAAQC,IAAR,qBAA0BF,IAG1B,IAAMG,EAAiB,CACnB,YACA,QACA,QACA,aACA,QACA,OACA,cACA,UACA,UAUEC,EAAY,uCAAG,WAAOC,GAAP,SAAAC,EAAA,+EACVD,EAAKE,QAAO,SAACC,GAChB,MAAiC,SAA1BA,EAAGC,KAAKC,kBAFF,2CAAH,sDAMZC,EAAmB,uCAAG,WAAON,GAAP,SAAAC,EAAA,+EACjBD,EAAKE,QAAO,SAACC,GAChB,MAAiC,gBAA1BA,EAAGC,KAAKC,kBAFK,2CAAH,sDAMnBE,EAAO,SAACC,GACV,OAAO,IAAIC,SAAQ,SAAAC,GACfC,WAAWD,EAASF,OAKfI,EAAc,uCAAG,WAAOC,GAAP,mBAAAZ,EAAA,6DAGpBa,EAAuB,IAAIC,IAC7B,IAAIC,IAAkB,CAAEC,SAAU,CAAE,4BAA6BxB,KAAUE,GAGzEuB,EAAeL,GAAOzB,IAPF,SAUH0B,EAAqBK,aAAaD,EAAc,CAAEpB,mBAV/C,UAUpBsB,EAVoB,QAatBrB,EAAaqB,EAASpB,QAASM,EAAoBc,EAASpB,MAbtC,gCAcAqB,EAAgBP,EAAsBI,GAdtC,OActBE,EAASE,KAda,uCAkBnB,CAAC,aAAE,IAAOJ,GAAiBE,KAlBR,4CAAH,sDAqBrBC,EAAe,uCAAG,WAAOE,EAAQV,GAAf,yBAAAZ,EAAA,6DAEduB,EAAmB,YACH,SAHF,SAKDD,EAAOE,KAAKZ,GALX,OAKhBa,EALgB,OAMhBC,EAAcD,EAAOE,kBAAkBC,MAAM,KAAKC,OAAO,GAAG,GAI1DC,EAAQC,KAAKC,MACnBrC,QAAQC,IAAR,UAAekC,EAAf,uBAAyBL,SAAzB,aAAyB,EAAQQ,OAAjC,MAXoB,UAabR,EAAOQ,SAAWV,EAbL,kCAcVjB,EAAK,KAdK,eAehBX,QAAQC,IAAR,UAAemC,KAAKC,MAAQF,EAA5B,uBAAsCL,SAAtC,aAAsC,EAAQQ,OAA9C,MAfgB,UAgBDX,EAAOY,cAAcR,GAhBpB,QAgBhBD,EAhBgB,uDAqBbA,EAAOU,eArBM,4CAAH,wD,qBCvET1C,uRAAY2C,oBACP3C,uRAAY4C,yBCiFdC,MA/Ef,WAAgB,IAAD,EAE2BC,mBAAS,MAFpC,mBAENC,EAFM,KAEQC,EAFR,OAGmBF,mBAAS,IAH5B,mBAGNpB,EAHM,KAGIuB,EAHJ,OAIuBH,oBAAS,GAJhC,mBAINI,EAJM,KAIMC,EAJN,KAMPC,EAAe,SAACC,GACpBL,EAAgBK,EAAEC,OAAOC,QAErBC,EAAmB,SAACH,GAGxBF,GAAc,GACdF,EAAY,IAEZ/B,EAAe6B,GAAgB,MAAMU,MAAK,SAACC,GAEzCT,EAAYS,GACZV,EAAgB,IAChBG,GAAc,OAUZQ,EAAiB,kBACrB,gCACE,0DACA,8BAAK,qBAAKC,IAAKlC,EAAS,GAAGmC,IAAKC,OAAO,MAAMC,OAAO,SAP/BC,EAQJtC,EAPX,8BAAK,8BAAMuC,KAAKC,UAAUF,EAAM,KAAM,WADxB,IAACA,GAYnBG,EAAU,WACd,OACA,gCACE,gDACEjB,GACA,gCACE,gCACE,wCACA,uBAAOkB,KAAK,OAAOC,YAAY,4DAA4DC,KAAK,KAAKC,SAAUnB,OAEjH,wBAAQoB,QAAShB,EAAjB,wBAGHN,GAAc,6CACf,uBACCxB,EAAS5B,OAAS,GAAK6D,QAKtBc,EAAc,WAClB,OACE,uFAYJ,OACE,8BFnDwB,WACxB,IAAMzC,EAAUjC,EAAID,OAAS,GAAKG,EAASH,OAAS,EAEpD,OADAI,QAAQC,IAAR,yBAA8B6B,IACvBA,EEwCO0C,GAEL,cAACP,EAAD,IAEF,cAACM,EAAD,OC9DIE,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBpB,MAAK,YAAkD,IAA/CqB,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOF,GACPG,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAQN,OCDdO,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1BZ,K","file":"static/js/main.a26c98ac.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.103b5fa1.svg\";","const describeURL = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/celebrities.jpg';\r\nconst categoryURLImage = 'https://moderatorsampleimages.blob.core.windows.net/samples/sample16.png';\r\nconst tagsURL = 'https://moderatorsampleimages.blob.core.windows.net/samples/sample16.png';\r\nconst objectURL = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-node-sdk-samples/master/Data/image.jpg';\r\nconst brandURLImage = 'https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/images/red-shirt-logo.jpg';\r\nconst facesImageURL = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/faces.jpg';\r\nconst printedTextSampleURL = 'https://moderatorsampleimages.blob.core.windows.net/samples/sample2.jpg';\r\nconst multiLingualTextURL = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/MultiLingual.png';\r\nconst adultURLImage = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/celebrities.jpg';\r\nconst colorURLImage = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/celebrities.jpg';\r\n// don't use with picture analysis\r\n// eslint-disable-next-line\r\nconst mixedMultiPagePDFURL = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/MultiPageHandwrittenForm.pdf';\r\nconst domainURLImage = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/landmark.jpg';\r\nconst typeURLImage = 'https://raw.githubusercontent.com/Azure-Samples/cognitive-services-python-sdk-samples/master/samples/vision/images/make_things_happen.jpg';\r\n\r\nconst DefaultImages = [\r\n    describeURL,\r\n    categoryURLImage,\r\n    tagsURL,\r\n    objectURL,\r\n    brandURLImage,\r\n    facesImageURL,\r\n    adultURLImage,\r\n    colorURLImage,\r\n    domainURLImage,\r\n    typeURLImage,\r\n    printedTextSampleURL,\r\n    multiLingualTextURL,\r\n    //mixedMultiPagePDFURL\r\n];\r\n\r\nconst RandomImageUrl = () => {\r\n    return DefaultImages[Math.floor(Math.random() * Math.floor(DefaultImages.length))];\r\n}\r\n\r\nexport default RandomImageUrl;","// Azure SDK client libraries\r\nimport { ComputerVisionClient } from '@azure/cognitiveservices-computervision';\r\nimport { ApiKeyCredentials } from '@azure/ms-rest-js';\r\n\r\n// List of sample images to use in demo\r\nimport RandomImageUrl from './DefaultImages';\r\n\r\n// Authentication requirements\r\nconst key = process.env.REACT_APP_ComputerVisionKey;\r\nconst endpoint = process.env.REACT_APP_ComputerVisionEndPoint;\r\n\r\nconsole.log(`key = ${key}`)\r\nconsole.log(`endpoint = ${endpoint}`)\r\n\r\n// Cognitive service features\r\nconst visualFeatures = [\r\n    \"ImageType\",\r\n    \"Faces\",\r\n    \"Adult\",\r\n    \"Categories\",\r\n    \"Color\",\r\n    \"Tags\",\r\n    \"Description\",\r\n    \"Objects\",\r\n    \"Brands\"\r\n];\r\n\r\nexport const isConfigured = () => {\r\n    const result = (key.length > 0 && endpoint.length > 0) ? true : false;\r\n    console.log(`isConfigured = ${result}`)\r\n    return result;\r\n}\r\n\r\n// Computer Vision detected Printed Text\r\nconst includesText = async (tags) => {\r\n    return tags.filter((el) => {\r\n        return el.name.toLowerCase() === \"text\";\r\n    });\r\n}\r\n// Computer Vision detected Handwriting\r\nconst includesHandwriting = async (tags) => {\r\n    return tags.filter((el) => {\r\n        return el.name.toLowerCase() === \"handwriting\";\r\n    });\r\n}\r\n// Wait for text detection to succeed\r\nconst wait = (timeout) => {\r\n    return new Promise(resolve => {\r\n        setTimeout(resolve, timeout);\r\n    });\r\n}\r\n\r\n// Analyze Image from URL\r\nexport const computerVision = async (url) => {\r\n\r\n    // authenticate to Azure service\r\n    const computerVisionClient = new ComputerVisionClient(\r\n        new ApiKeyCredentials({ inHeader: { 'Ocp-Apim-Subscription-Key': key } }), endpoint);\r\n\r\n    // get image URL - entered in form or random from Default Images\r\n    const urlToAnalyze = url || RandomImageUrl();\r\n    \r\n    // analyze image\r\n    const analysis = await computerVisionClient.analyzeImage(urlToAnalyze, { visualFeatures });\r\n\r\n    // text detected - what does it say and where is it\r\n    if (includesText(analysis.tags) || includesHandwriting(analysis.tags)) {\r\n        analysis.text = await readTextFromURL(computerVisionClient, urlToAnalyze);\r\n    }\r\n\r\n    // all information about image\r\n    return [{ \"URL\": urlToAnalyze, ...analysis}];\r\n}\r\n// analyze text in image\r\nconst readTextFromURL = async (client, url) => {\r\n\r\n    const STATUS_SUCCEEDED = \"succeeded\";\r\n    const STATUS_FAILED = \"failed\"\r\n    \r\n    let result = await client.read(url);\r\n    let operationID = result.operationLocation.split('/').slice(-1)[0];\r\n\r\n    // Wait for read recognition to complete\r\n    // result.status is initially undefined, since it's the result of read\r\n    const start = Date.now();\r\n    console.log(`${start} -${result?.status} `);\r\n    \r\n    while (result.status !== STATUS_SUCCEEDED) {\r\n        await wait(500);\r\n        console.log(`${Date.now() - start} -${result?.status} `);\r\n        result = await client.getReadResult(operationID);\r\n    }\r\n    \r\n    // Return the first page of result. \r\n    // Replace[0] with the desired page if this is a multi-page file such as .pdf or.tiff.\r\n    return result.analyzeResult; \r\n}","import * as Speech from \"microsoft-cognitiveservices-speech-sdk\";\r\n\r\n// Authentication requirements\r\nconst key = process.env.REACT_APP_SpeechKey || \"fbfab2b80910435e91e080bebbf2a80d\";\r\nconst endpoint = process.env.REACT_APP_SpeechEndPoint || 'https://eastus.api.cognitive.microsoft.com/sts/v1.0/issuetoken';\r\n\r\nexport const isConfigured = () => {\r\n    \r\n    const result = (key.length > 0 && endpoint.length > 0) ? true : false;\r\n    console.log(`isConfigured = ${result}`)\r\n    return result;\r\n}\r\n\r\nexport const synthesizeSpeech = async (text, fileName) => {\r\n    \r\n    try {\r\n        const speechConfig = Speech.SpeechConfig.fromEndpoint(endpoint, key);\r\n        const audioConfig = Speech.AudioConfig.fromAudioFileOutput(fileName /*\"path-to-file.wav\"*/);\r\n\r\n        const synthesizer = new Speech.SpeechSynthesizer(speechConfig, audioConfig);\r\n        const result = await synthesizer.speakTextAsync(text);\r\n        synthesizer.close();\r\n        return result;\r\n    } catch (ex) {\r\n        console.log(ex.message);\r\n    }\r\n}","import React, { useState } from 'react';\nimport logo from './logo.svg';\nimport './App.css';\nimport { computerVision, isConfigured as ComputerVisionIsConfigured} from './VisualAI';\nimport { isConfigured as SpeechIsConfigured, synthesizeSpeech } from './SpeechAI';\n\nfunction App() {\n\n  const [fileSelected, setFileSelected] = useState(null);\n  const [analysis, setAnalysis] = useState({});\n  const [processing, setProcessing] = useState(false);\n  \n  const handleChange = (e) => {\n    setFileSelected(e.target.value)\n  }\n  const onFileUrlEntered = (e) => {\n\n    // hold UI\n    setProcessing(true);\n    setAnalysis({});\n\n    computerVision(fileSelected || null).then((items) => {\n      // reset state/form\n      setAnalysis(items);\n      setFileSelected(\"\");\n      setProcessing(false);\n    });\n\n  };\n\n  // Display JSON data in readable format\n  const PrettyPrintJson = (data) => {\n    return (<div><pre>{JSON.stringify(data, null, 2)}</pre></div>);\n  }\n\n  const DisplayResults = () => (\n    <div>\n      <h2>Computer Vision Analysis</h2>\n      <div><img src={analysis[0].URL} height=\"200\" border=\"1\" /></div>\n      {PrettyPrintJson(analysis)}\n    </div>\n  );\n  \n  const Analyze = () => {\n    return (\n    <div>\n      <h1>Analyze image</h1>\n      {!processing &&\n        <div>\n          <div>\n            <label>URL</label>\n            <input type=\"text\" placeholder=\"Enter URL or leave empty for random image from collection\" size=\"50\" onChange={handleChange}></input>\n          </div>\n          <button onClick={onFileUrlEntered}>Analyze</button>\n        </div>\n      }\n      {processing && <div>Processing</div>}\n      <hr />\n      {analysis.length > 0 && DisplayResults()}\n      </div>\n    )\n  }\n  \n  const CantAnalyze = () => {\n    return (\n      <div>Key and/or endpoint not configured in ./VisualAI.js</div>\n    )\n  }\n  \n  function Render() {\n    const ready = ComputerVisionIsConfigured();\n    if (ready) {\n      return <Analyze />;\n    }\n    return <CantAnalyze />;\n  }\n\n  return (\n    <div>\n      {Render()}\n    </div>\n    \n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}